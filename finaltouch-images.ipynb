{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-04-12T14:41:46.161757Z","iopub.status.busy":"2024-04-12T14:41:46.160927Z","iopub.status.idle":"2024-04-12T14:42:01.730567Z","shell.execute_reply":"2024-04-12T14:42:01.729152Z","shell.execute_reply.started":"2024-04-12T14:41:46.161714Z"},"trusted":true},"outputs":[],"source":["!pip install -q ultralytics"]},{"cell_type":"code","execution_count":33,"metadata":{"execution":{"iopub.execute_input":"2024-04-12T15:01:14.553363Z","iopub.status.busy":"2024-04-12T15:01:14.552854Z","iopub.status.idle":"2024-04-12T15:01:14.558835Z","shell.execute_reply":"2024-04-12T15:01:14.557896Z","shell.execute_reply.started":"2024-04-12T15:01:14.553326Z"},"trusted":true},"outputs":[],"source":["import os\n","import json\n","import math\n","import torch\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","import cv2\n","from PIL import Image\n","\n","from ultralytics import YOLO"]},{"cell_type":"code","execution_count":32,"metadata":{"execution":{"iopub.execute_input":"2024-04-12T15:01:12.116457Z","iopub.status.busy":"2024-04-12T15:01:12.115904Z","iopub.status.idle":"2024-04-12T15:01:12.124315Z","shell.execute_reply":"2024-04-12T15:01:12.122681Z","shell.execute_reply.started":"2024-04-12T15:01:12.116420Z"},"trusted":true},"outputs":[],"source":["def get_central_band(image_path, band_width):\n","    image = cv2.imread(image_path)\n","    height, width, _ = image.shape\n","\n","    # Calculer la position centrale de la bande\n","    start_x = (width - band_width) // 2\n","    end_x = start_x + band_width\n","\n","    # Extraire la bande centrale de l'image\n","    central_band = image[:, start_x:end_x]\n","\n","    return central_band"]},{"cell_type":"code","execution_count":34,"metadata":{"execution":{"iopub.execute_input":"2024-04-12T15:01:16.420147Z","iopub.status.busy":"2024-04-12T15:01:16.419393Z","iopub.status.idle":"2024-04-12T15:01:16.425302Z","shell.execute_reply":"2024-04-12T15:01:16.424080Z","shell.execute_reply.started":"2024-04-12T15:01:16.420108Z"},"trusted":true},"outputs":[],"source":["def read_this(image_file, gray_scale=False):\n","    image_src = cv2.imread(image_file)\n","    if gray_scale:\n","        image_src = cv2.cvtColor(image_src, cv2.COLOR_BGR2GRAY)\n","\n","    return image_src"]},{"cell_type":"code","execution_count":35,"metadata":{"execution":{"iopub.execute_input":"2024-04-12T15:01:27.639264Z","iopub.status.busy":"2024-04-12T15:01:27.638777Z","iopub.status.idle":"2024-04-12T15:01:27.647339Z","shell.execute_reply":"2024-04-12T15:01:27.645723Z","shell.execute_reply.started":"2024-04-12T15:01:27.639229Z"},"trusted":true},"outputs":[],"source":["def enhance_contrast(image_matrix, bins=256):\n","    image_flattened = image_matrix.flatten()\n","    image_hist = np.zeros(bins)\n","\n","    # frequency count of each pixel\n","    for pix in image_matrix:\n","        image_hist[pix] += 1\n","\n","    # cummulative sum\n","    cum_sum = np.cumsum(image_hist)\n","    norm = (cum_sum - cum_sum.min()) * 255\n","    # normalization of the pixel values\n","    n_ = cum_sum.max() - cum_sum.min()\n","    uniform_norm = norm / n_\n","    uniform_norm = uniform_norm.astype('int')\n","\n","    # flat histogram\n","    image_eq = uniform_norm[image_flattened]\n","    # reshaping the flattened matrix to its original shape\n","    image_eq = np.reshape(a=image_eq, newshape=image_matrix.shape)\n","\n","    return image_eq"]},{"cell_type":"code","execution_count":36,"metadata":{"execution":{"iopub.execute_input":"2024-04-12T15:01:42.892420Z","iopub.status.busy":"2024-04-12T15:01:42.891415Z","iopub.status.idle":"2024-04-12T15:01:42.902533Z","shell.execute_reply":"2024-04-12T15:01:42.901243Z","shell.execute_reply.started":"2024-04-12T15:01:42.892370Z"},"trusted":true},"outputs":[],"source":["def equalize_this(image_file, with_plot=False, gray_scale=False, bins=256):\n","    image_src = read_this(image_file=image_file, gray_scale=gray_scale)\n","    if not gray_scale:\n","        r_image = image_src[:, :, 0]\n","        g_image = image_src[:, :, 1]\n","        b_image = image_src[:, :, 2]\n","\n","        r_image_eq = enhance_contrast(image_matrix=r_image)\n","        g_image_eq = enhance_contrast(image_matrix=g_image)\n","        b_image_eq = enhance_contrast(image_matrix=b_image)\n","\n","        image_eq = np.dstack(tup=(r_image_eq, g_image_eq, b_image_eq))\n","        cmap_val = None\n","    else:\n","        image_eq = enhance_contrast(image_matrix=image_src)\n","        cmap_val = 'gray'\n","\n","    if with_plot:\n","        fig = plt.figure(figsize=(10, 20))\n","\n","        ax1 = fig.add_subplot(2, 2, 1)\n","        ax1.axis(\"off\")\n","        ax1.title.set_text('Original')\n","        ax2 = fig.add_subplot(2, 2, 2)\n","        ax2.axis(\"off\")\n","        ax2.title.set_text(\"Equalized\")\n","\n","        ax1.imshow(image_src, cmap=cmap_val)\n","        ax2.imshow(image_eq, cmap=cmap_val)\n","        return True, image_eq\n","    return image_eq"]},{"cell_type":"code","execution_count":37,"metadata":{"execution":{"iopub.execute_input":"2024-04-12T15:05:01.964185Z","iopub.status.busy":"2024-04-12T15:05:01.963653Z","iopub.status.idle":"2024-04-12T15:05:01.970381Z","shell.execute_reply":"2024-04-12T15:05:01.969058Z","shell.execute_reply.started":"2024-04-12T15:05:01.964147Z"},"trusted":true},"outputs":[],"source":["def adjust_brightness_contrast(image, alpha, beta):\n","    return cv2.addWeighted(image, alpha, image, 0, beta)"]},{"cell_type":"code","execution_count":44,"metadata":{"execution":{"iopub.execute_input":"2024-04-12T15:27:39.012567Z","iopub.status.busy":"2024-04-12T15:27:39.012048Z","iopub.status.idle":"2024-04-12T15:27:39.025957Z","shell.execute_reply":"2024-04-12T15:27:39.024237Z","shell.execute_reply.started":"2024-04-12T15:27:39.012534Z"},"trusted":true},"outputs":[],"source":["def blur_img(img_path):\n","    maxblur = 9\n","    ramppow = 1\n","\n","    # read the input\n","    img = cv2.imread(img_path)\n","    height, width, _ = img.shape\n","    center = [height//2, width//2]\n","\n","    # blur the image to the maximum desired\n","    blur = cv2.GaussianBlur(img, (maxblur,maxblur), sigmaX=4000, sigmaY=4000)\n","\n","    # Note [:, None] and [None, :] specify which dimension that array will specify\n","    # Linear ramps are between 0 and 1 and have number of increments of width or height\n","    x = np.linspace(0, 1, width)[:, None]\n","    y = np.linspace(0, 1, height)[None, :]\n","\n","    # normalize the center coordinates between 0 and 1\n","    cx = center[1]/width\n","    cy = center[0]/height\n","\n","    # Set graylevel of gradient proportional to distance from center\n","    # Note multiply by sqrt(2) so 0 to 255 goes diagonally from center\n","    # Include non-linear ramp power function to blur faster as get farther from the center\n","    # Linear ramp would be ramppow=1\n","    ramppow = 1/ramppow\n","    gradient = np.sqrt((x-cx)**2 + (y-cy)**2)\n","    gradient = cv2.pow(gradient,ramppow)\n","\n","    gradient = (255*math.sqrt(2)*gradient).clip(0,255).astype(np.uint8)\n","    gradient = np.transpose(gradient)\n","    gradient = cv2.merge([gradient,gradient,gradient])\n","\n","    # Invert the gradient\n","    # gradient = 1 - gradient\n","\n","    # merge the input and blurred image using the gradient\n","    result = ((gradient*blur.astype(np.float64) + (1-gradient)*img.astype(np.float64))/255).clip(0,255).astype(np.uint8)\n","    \n","    return result"]},{"cell_type":"code","execution_count":41,"metadata":{"execution":{"iopub.execute_input":"2024-04-12T15:23:57.977329Z","iopub.status.busy":"2024-04-12T15:23:57.976856Z","iopub.status.idle":"2024-04-12T15:23:57.986657Z","shell.execute_reply":"2024-04-12T15:23:57.984946Z","shell.execute_reply.started":"2024-04-12T15:23:57.977295Z"},"trusted":true},"outputs":[],"source":["def mask_blur(results, bright_path, blur_path):\n","    \n","    mask_wm = results[0].masks.data[0]\n","\n","    image = cv2.imread(bright_path)\n","    image_blur = cv2.imread(blur_path)\n","\n","    # Redimensionner le tensor du masque pour qu'il corresponde aux dimensions de l'image\n","    image_shape = (np.array(image).shape[0], np.array(image).shape[1])\n","    \n","    mask_tensor = torch.nn.functional.interpolate(mask_wm.unsqueeze(0).unsqueeze(0), size=image_shape, mode='nearest')\n","    mask_tensor = mask_tensor.squeeze()\n","\n","    # Convertir le tensor en un masque binaire numpy\n","    mask_binary_np = (mask_tensor.cpu().detach().numpy() > 0.5).astype(np.uint8)\n","\n","    # Convertir les images en tableaux NumPy\n","    image_np = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","    image_blur_np = cv2.cvtColor(image_blur, cv2.COLOR_BGR2RGB)\n","\n","    # Appliquer le masque binaire sur l'image floue\n","    masked_image = np.copy(image_blur_np)\n","    masked_image[mask_binary_np == 1] = image_np[mask_binary_np == 1]\n","    \n","    return masked_image"]},{"cell_type":"code","execution_count":46,"metadata":{"execution":{"iopub.execute_input":"2024-04-12T15:35:09.649918Z","iopub.status.busy":"2024-04-12T15:35:09.649450Z","iopub.status.idle":"2024-04-12T15:36:03.589483Z","shell.execute_reply":"2024-04-12T15:36:03.588086Z","shell.execute_reply.started":"2024-04-12T15:35:09.649882Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["/kaggle/input/clean-mannequin/1_out.png\n","/kaggle/working/1_out_box.png\n","/kaggle/working/1_out_box_contrasted.png\n","/kaggle/working/1_out_box_contrasted_bright.png\n","/kaggle/working/1_out_box_contrasted_bright_blur.png\n","\n","image 1/1 /kaggle/working/1_out_box_contrasted_bright.png: 640x448 3 persons, 1 handbag, 1994.8ms\n","Speed: 3.4ms preprocess, 1994.8ms inference, 7.5ms postprocess per image at shape (1, 3, 640, 448)\n","/kaggle/working/1_out_box_contrasted_bright_blur_final.png\n","/kaggle/input/clean-mannequin/2_out.png\n","/kaggle/working/2_out_box.png\n","/kaggle/working/2_out_box_contrasted.png\n","/kaggle/working/2_out_box_contrasted_bright.png\n","/kaggle/working/2_out_box_contrasted_bright_blur.png\n","\n","image 1/1 /kaggle/working/2_out_box_contrasted_bright.png: 640x448 3 persons, 1944.4ms\n","Speed: 3.9ms preprocess, 1944.4ms inference, 6.0ms postprocess per image at shape (1, 3, 640, 448)\n","/kaggle/working/2_out_box_contrasted_bright_blur_final.png\n","/kaggle/input/clean-mannequin/3_out.png\n","/kaggle/working/3_out_box.png\n","/kaggle/working/3_out_box_contrasted.png\n","/kaggle/working/3_out_box_contrasted_bright.png\n","/kaggle/working/3_out_box_contrasted_bright_blur.png\n","\n","image 1/1 /kaggle/working/3_out_box_contrasted_bright.png: 640x448 4 persons, 2 handbags, 1988.2ms\n","Speed: 4.1ms preprocess, 1988.2ms inference, 10.0ms postprocess per image at shape (1, 3, 640, 448)\n","/kaggle/working/3_out_box_contrasted_bright_blur_final.png\n","/kaggle/input/clean-mannequin/4_out.png\n","/kaggle/working/4_out_box.png\n","/kaggle/working/4_out_box_contrasted.png\n","/kaggle/working/4_out_box_contrasted_bright.png\n","/kaggle/working/4_out_box_contrasted_bright_blur.png\n","\n","image 1/1 /kaggle/working/4_out_box_contrasted_bright.png: 640x448 1 person, 2042.6ms\n","Speed: 3.9ms preprocess, 2042.6ms inference, 3.4ms postprocess per image at shape (1, 3, 640, 448)\n","/kaggle/working/4_out_box_contrasted_bright_blur_final.png\n","/kaggle/input/clean-mannequin/5_out.png\n","/kaggle/working/5_out_box.png\n","/kaggle/working/5_out_box_contrasted.png\n","/kaggle/working/5_out_box_contrasted_bright.png\n","/kaggle/working/5_out_box_contrasted_bright_blur.png\n","\n","image 1/1 /kaggle/working/5_out_box_contrasted_bright.png: 640x448 7 persons, 1960.4ms\n","Speed: 3.5ms preprocess, 1960.4ms inference, 12.2ms postprocess per image at shape (1, 3, 640, 448)\n","/kaggle/working/5_out_box_contrasted_bright_blur_final.png\n"]}],"source":["folder_images = './clean-mannequin'\n","output_folder = './'\n","\n","# Load a model\n","model = YOLO('yolov8x-seg.pt')\n","\n","band_width = 3000  # Largeur de la bande centrale Ã  extraire\n","\n","for file in sorted(os.listdir(folder_images)):\n","    filename = os.path.join(folder_images, file)\n","    print(filename)\n","    \n","    central_band = get_central_band(filename, band_width)\n","    central_band = cv2.cvtColor(central_band, cv2.COLOR_BGR2RGB)\n","    \n","    box_path = os.path.join(output_folder, file).replace(\".png\", \"\") + \"_box.png\"\n","    cv2.imwrite(box_path, central_band)\n","    print(box_path)\n","    \n","    image_contrast = equalize_this(image_file=box_path, with_plot=False)\n","    \n","    contrast_path = box_path.replace(\".png\", \"\") + \"_contrasted.png\"\n","    cv2.imwrite(contrast_path, image_contrast)\n","    print(contrast_path)\n","    \n","    # Adjust brightness and contrast\n","    bright_image = adjust_brightness_contrast(image_contrast, 0.9, -20)\n","    \n","    bright_path = contrast_path.replace(\".png\", \"\") + \"_bright.png\"\n","    cv2.imwrite(bright_path, bright_image)\n","    print(bright_path)\n","    \n","    blur_image = blur_img(bright_path)\n","    \n","    blur_path = bright_path.replace(\".png\", \"\") + \"_blur.png\"\n","    cv2.imwrite(blur_path, blur_image)\n","    print(blur_path)\n","    \n","    # Predict with the model\n","    results = model(bright_path)\n","    \n","    final_image = mask_blur(results, bright_path, blur_path)\n","    \n","    final_path = blur_path.replace(\".png\", \"\") + \"_final.png\"\n","    cv2.imwrite(final_path, final_image)\n","    print(final_path)"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":4784001,"sourceId":8100935,"sourceType":"datasetVersion"}],"dockerImageVersionId":30684,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
